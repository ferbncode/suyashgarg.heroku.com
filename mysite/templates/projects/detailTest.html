{% extends 'base.html' %}
{%block head%}
{{super()}}
{%endblock%}
{%block body%}
{{super()}}
{%endblock%}
{%block maindiv%}
<p><font size="6">AutoDrive</font></p>
May 2016
<hr>
<br/>

<h2>What is Autonomous Driving?</h2>

An autonomous car (driverless car, self-driving car, robotic car) is a vehicle that is capable of sensing its environment and navigating without human input.
I am just making a simple attempt to train my raspberry pi car to autodrive on my home floor. (just an approach to put my raspberry pi to some use). I am using
two approaches and then i plan to combine both the approaches to make a self-driving car that runs well through my house. The applications of solving this problem are
sempiternal (like a robo-nurse in a hospital environment). Let us start up with the supervised approach first. Then we would go on with reinforcement learning approach and then try to combine them both. The whole project may be quite naive. (It's my first proper ml project). All suggestions are welcome. :-).
Find the project code <a href='https://github.com/ferbncode/AutoDrive'>here</a>.
<br/><br/>
<div align="center">
<img style="max-width:350px;border-radius:3%" src="{{url_for('static', filename='images/car3.jpg')}}" alt="Image of the Raspberry Pi Car">
<p>The Raspberry Pi Car</p>
</div>
<h3><u>Supervised Approach</u></h3>

<h4>The Pipeline.</h4>

Its good to have a proper pipeline before starting with a proper machine learning problem. Let us just list the pipeline steps and then discuss steps one by one. A ceil test may later help me determine the step that needs more attention. This is a supervised learning approach pipeline.
<ul>
<li>Getting the Dataset</li>
<li>Processing Dataset and Labelling it.</li>
<li>Deciding features</li>
<li>Choosing a suitable evaluation metric</li>
<li>Choosing a suitable classifier</li>
<li>Improve and choosing actions for the raspberry pi car</li>
</ul>

<h4>Getting Dataset, Processing and Labelling the Dataset.</h4>

The CMU /VSAC Image DataBase was used to gather the road images. Different road situations were present in the dataset. The dataset presented by CMU/VSAC is taken from various NavLabs. Find the dataset <a href="http://vasc.ri.cmu.edu/idb/html/road">here</a>.
<br/>
<br/>
<div class="ui tiny images" align="center">
	<img class="ui image" src="{{url_for('static', filename='images/1.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/2.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/3.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/4.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/5.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/6.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/7.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/8.png')}}">
</div>
As you can see that there are many different images taken in the dataset. An important thing to note is that for the raspberry pi car, the roads would seem similar (atleast, that holds true for my home floor :) )
<br/>
<br/>
<p>Applying K-Means to the above dataset generates us with the following sort of images:
<br/>

<div class="ui tiny images" align="center">
	<img class="ui image" src="{{url_for('static', filename='images/A8.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A1.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A6.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A5.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A3.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A2.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A4.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A7.png')}}">
</div>

<strong>Observation from KMeans Clustering</strong> : The blue intensity of the two clusters in all the above clustered images presents the best distinction between the road and the ground. This comes straight from the ALVINN paper, however, having some concreteness is quite important before moving forward. Find the K-Means clustering implementation <a href="https://github.com/ferbncode/No-Free-Lunch/tree/master/Image%20Compression">here</a>.
<br/><br/>
Though i am not a very good driver, i labelled the dataset by myself. So, the source of all noise is my driving!. This is the point that needs most improvement. I know its a bit naive but i couldnot have a very large dataset (like ALVINN does by noting image and action taken by actual driver) since i am using the raspberry pi. Also, this is just a part of the reinforcement learner, so that the initial random action is not very random (and based on this supervised learner).
<br/>
<strong><u>Dataset</u></strong>: So, the blue intensities of all images are taken as the dataset elements with my choosen actions. The final dataset has 458 examples. More features need to be used for scaling the problem better. These features could be input from ultrasonic sensors of nearby objects. (The reinforcement learner is based on that). 
<br/>
<h4>A Suitable Evaluation Metric</h4>
The dataset is too biased. Most of the acions(labels) are forward. Thus accuracy would not be a very good method for evaluation of the learning algorithm. For biased dataset F1 score is a very good evaluation metric. For recalling,
<br/><br/>
<div align="center">
<font size=2>
	F1 Score = 2(precision)(recall)/(precision + recall)
	<br/>		
</font>
</div>
<h4>Choosing a suitable classifier</h4>
Though an svm is a good classifier for such a small dataset with so many features, a decision tree classifier with maximum depth of 4 brought a better F1 score on the above dataset. I know this part needs to be improved and a k-cross validation would be a better thing to do and choose classifier. However, based on info gain, the decision tree classifier picks up the pixels that most precisely determine the action. This is poor due to noise and will be improved further. 

<h4>Choosing Actions for the Raspberry PI</h4>

<p>The output of the classifier is the action to be taken by the raspberry pi car. The raspberry pi car is made using the L298N motor driver. Speed would have been a good thing to learn. However this initial code of the project doesnot have this support. The pi instructs the motor driver to choose specific action. It takes a picture of the road every 2 seconds and then decides accordingly for the next 2 seconds. This whole model works good on the test set with a F1 score of 0.78. This can be further improved. This way the car is able to take action and act accordingly over my floor road :-). </p>


<br/>
<h3><u>Reinforcement Learning</u></h3>

{%endblock%}