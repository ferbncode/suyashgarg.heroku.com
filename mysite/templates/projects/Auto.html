{% extends 'base.html' %}
{%block title%}Suyash Garg | Projects{%endblock%}
{%block head%}
{{super()}}
{%endblock%}
{%block body%}
{{super()}}
{%endblock%}
{%block maindiv%}
<p><font size="6">AutoDrive</font></p>
{{project_detail['date']}}
<hr>
<br/>

<h2>What is Autonomous Driving?</h2>

An autonomous car (driverless car, self-driving car, robotic car) is a vehicle that is capable of sensing its environment and navigating without human input.
I am just making a simple attempt to train my raspberry pi car to autodrive on my home floor. (just an approach to put my raspberry pi to some use). I am using
two approaches and then i plan to combine both the approaches to make a self-driving car that runs well through my house. The applications of solving this problem are
sempiternal (like a robo-nurse in a hospital environment). Let us start up with the supervised approach first. Then we would go on with reinforcement learning approach and then try to combine them both. The whole project may be quite naive. (It's my first proper ml project). All suggestions are welcome. :-).
Find the project code <a href='https://github.com/ferbncode/AutoDrive'>here</a>.
<br/><br/>
<div align="center">
<img class="ui image"style="max-width:350px;border-radius:3%" src="{{url_for('static', filename='images/car3.jpg')}}" alt="Image of the Raspberry Pi Car">
<p>The Raspberry Pi Car</p>
</div>
<h3><u>Supervised Approach</u></h3>

<h4>The Pipeline.</h4>

Its good to have a proper pipeline before starting with a proper machine learning problem. Let us just list the pipeline steps and then discuss steps one by one. A ceil test may later help me determine the step that needs more attention. This is a supervised learning approach pipeline.
<ul>
<li>Getting the Dataset</li>
<li>Processing Dataset and Labelling it.</li>
<li>Deciding features</li>
<li>Choosing a suitable evaluation metric</li>
<li>Choosing a suitable classifier</li>
<li>Improve and choosing actions for the raspberry pi car</li>
</ul>

<h4>Getting Dataset, Processing and Labelling the Dataset.</h4>

The CMU /VSAC Image DataBase was used to gather the road images. Different road situations were present in the dataset. The dataset presented by CMU/VSAC is taken from various NavLabs. Find the dataset <a href="http://vasc.ri.cmu.edu/idb/html/road">here</a>.
<br/>
<br/>
<div class="ui tiny images" align="center">
	<img class="ui image" src="{{url_for('static', filename='images/1.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/2.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/3.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/4.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/5.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/6.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/7.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/8.png')}}">
</div>
As you can see that there are many different images taken in the dataset. An important thing to note is that for the raspberry pi car, the roads would seem similar (atleast, that holds true for my home floor :) )
<br/>
<br/>
<p>Applying K-Means to the above dataset generates us with the following sort of images:
<br/>

<div class="ui tiny images" align="center">
	<img class="ui image" src="{{url_for('static', filename='images/A8.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A1.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A6.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A5.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A3.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A2.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A4.png')}}">
	<img class="ui image" src="{{url_for('static', filename='images/A7.png')}}">
</div>

<strong>Observation from KMeans Clustering</strong> : The blue intensity of the two clusters in all the above clustered images presents the best distinction between the road and the ground. This comes straight from the ALVINN paper, however, having some concreteness is quite important before moving forward. Find the K-Means clustering implementation <a href="https://github.com/ferbncode/No-Free-Lunch/tree/master/Image%20Compression">here</a>.
<br/><br/>
Though i am not a very good driver, i labelled the dataset by myself. So, the source of all noise is my driving!. This is the point that needs most improvement. I know its a bit naive but i couldnot have a very large dataset (like ALVINN does by noting image and action taken by actual driver) since i am using the raspberry pi. Also, this is just a part of the reinforcement learner, so that the initial random action is not very random (and based on this supervised learner).
<br/>
<strong><u>Dataset</u></strong>: So, the blue intensities of all images are taken as the dataset elements with my choosen actions. The final dataset has 458 examples. More features need to be used for scaling the problem better. These features could be input from ultrasonic sensors of nearby objects. (The reinforcement learner is based on that). 
<br/>
<h4>A Suitable Evaluation Metric</h4>
The dataset is too biased. Most of the acions(labels) are forward. Thus accuracy would not be a very good method for evaluation of the learning algorithm. For biased dataset F1 score is a very good evaluation metric. For recalling,
<br/><br/>
<div align="center">
<font size=2>
	F1 Score = 2(precision)(recall)/(precision + recall)
	<br/>		
</font>
</div>
<h4>Choosing a suitable classifier</h4>
Though an svm is a good classifier for such a small dataset with so many features, a decision tree classifier with maximum depth of 4 brought a better F1 score on the above dataset. I know this part needs to be improved and a k-cross validation would be a better thing to do and choose classifier. However, based on info gain, the decision tree classifier picks up the pixels that most precisely determine the action. This is poor due to noise and will be improved further. 

<h4>Choosing Actions for the Raspberry PI</h4>

<p>The output of the classifier is the action to be taken by the raspberry pi car. The raspberry pi car is made using the L298N motor driver. Speed would have been a good thing to learn. However this initial code of the project doesnot have this support. The pi instructs the motor driver to choose specific action. It takes a picture of the road every 2 seconds and then decides accordingly for the next 2 seconds. This whole model works good on the test set with a F1 score of 0.78. This can be further improved. This way the car is able to take action and act accordingly over my floor road :-). </p>


<br/>
<h3><u>Reinforcement Learning</u></h3>

This project of making a smartcab using Q-learning was part of the Udacity ML course i have taken. Though they provided most of the code, the Q-learning part was to be implemented. I am working on the environment part and a map editor, so that larger environments can be learnt by the driving agent. 

<br/>
<div align="center">
<img style="max-width:350px;border-radius:3%" src = "{{url_for('static', filename='images/agent.png')}}">
</div>
<br/>
This learner takes every step as an intersection. There may be a car coming form any of the sides and there is a traffic light at each of the intersections. The route planner gives the car the next state situation and the car learns the optimal policy via Q-learning. There is a destination assigned and a deadline to reach destination given to the car. Learn more on Q-learning <a href="https://www.cs.cmu.edu/~tom/10701_sp11/slides/MDPs_RL_04_26_2011-ann.pdf">here</a>.
State includes the inputs and the location of the next waypoint.<br/>
	State Variables:<br/>
	<ul><li>Light</li>
		<li>Ongoing - Any car coming from front?</li>
		<li>Left - Any car coming from left?</li>
		<li>Right - Any car coming from right?</li>
		<li>Next Waypoint</li></ul>
The deadline is not important variable to decide the state of the red car. Seeing all such states (as defined above) will help the Q-learning algorithm to visit every possible condition the car can see (after a quite long training time). <br/>

Q-learning solves the Bellman equation and chooses the best action according to Q-values. The Q values are updated as:	
<div align="center">
Q(s,a) = (1-alpha)*Q(s,a) + alpha*(X)<br/>
X = reward + gamma*max(a')Q(s',a')
</div>
<br/>
 By application of Q-learning, reaches the destination without breaking rules and making use of the shortest path. The point is that the actions are firstly chosen randomly. Now,  the max(a) Q(s, a ) gives us the utility of the state we are in. Thus, after the agent has seen all actions related to a state the agent can then choose the best action. That would be moving closer to the optimal policy. The agent gets bigger reward for reaching the destination, thus this strengthens the optimal policy more. The presence of the next_waypoint in the state helps the agent to take the best action trying to take the shortest path and avoiding any accidents. Any illegal action will make the agent get negative rewards. Thus the policy will be changing (if necessary, that is if the Q-values changes) and getting closer to the optimal policy.

 <h3><u>Combining the above two approaches</u></h3>

 The agent in the reinforcement part works randomly, atleast initially. Getting more ultrasonic sensors would make the pi car closer to that in the simulator. The supervised learner pops in where the random part comes in. Combining these techniques may overcome some of their individual cons. Initially the action is chosen by the supervised learner(rather than by random choice). 

<br/>
<br/>
The car works fine sometimes but terrible too. The project is kind of naive and will be improved as i learn more things. Any suggestions and comments are welcome. This project would encourage me to learn more machine learning properly. :-)


{%endblock%}
